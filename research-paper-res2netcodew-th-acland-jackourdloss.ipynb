{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\nimport keras.backend as K\n\n\n\n\ndef up_and_concate(down_layer, layer, data_format='channels_first'):\n    if data_format == 'channels_first':\n        in_channel = down_layer.get_shape().as_list()[1]\n    else:\n        in_channel = down_layer.get_shape().as_list()[3]\n\n    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n\n    if data_format == 'channels_first':\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n    else:\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n\n    concate = my_concat([up, layer])\n\n    return concate\n\n\ndef attention_up_and_concate(down_layer, layer, data_format='channels_first'):\n    if data_format == 'channels_first':\n        in_channel = down_layer.get_shape().as_list()[1]\n    else:\n        in_channel = down_layer.get_shape().as_list()[3]\n\n    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n\n    layer = attention_block_2d(x=layer, g=up, inter_channel=in_channel // 4, data_format=data_format)\n\n    if data_format == 'channels_first':\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n    else:\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n\n    concate = my_concat([up, layer])\n    return concate\n\n\ndef attention_block_2d(x, g, inter_channel, data_format='channels_first'):\n    # theta_x(?,g_height,g_width,inter_channel)\n\n    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(x)\n\n    # phi_g(?,g_height,g_width,inter_channel)\n\n    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(g)\n\n    # f(?,g_height,g_width,inter_channel)\n\n    f = Activation('relu')(add([theta_x, phi_g]))\n\n    # psi_f(?,g_height,g_width,1)\n\n    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format=data_format)(f)\n\n    rate = Activation('sigmoid')(psi_f)\n\n    # rate(?,x_height,x_width)\n\n    # att_x(?,x_height,x_width,x_channel)\n\n    att_x = multiply([x, rate])\n\n    return att_x\n\n\ndef res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n\n              padding='same', data_format='channels_first'):\n    if data_format == 'channels_first':\n        input_n_filters = input_layer.get_shape().as_list()[1]\n    else:\n        input_n_filters = input_layer.get_shape().as_list()[3]\n\n    layer = input_layer\n    for i in range(2):\n        layer = Conv2D(out_n_filters // 4, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n        if batch_normalization:\n            layer = BatchNormalization()(layer)\n        layer = Activation('relu')(layer)\n        layer = Conv2D(out_n_filters // 4, kernel_size, strides=stride, padding=padding, data_format=data_format)(layer)\n        layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n\n    if out_n_filters != input_n_filters:\n        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n            input_layer)\n    else:\n        skip_layer = input_layer\n    out_layer = add([layer, skip_layer])\n    return out_layer\n\n\n# Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\ndef rec_res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n\n                  padding='same', data_format='channels_first'):\n    if data_format == 'channels_first':\n        input_n_filters = input_layer.get_shape().as_list()[1]\n    else:\n        input_n_filters = input_layer.get_shape().as_list()[3]\n\n    if out_n_filters != input_n_filters:\n        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n            input_layer)\n    else:\n        skip_layer = input_layer\n\n    layer = skip_layer\n    for j in range(2):\n\n        for i in range(2):\n            if i == 0:\n\n                layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n                    layer)\n                if batch_normalization:\n                    layer1 = BatchNormalization()(layer1)\n                layer1 = Activation('relu')(layer1)\n            layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n                add([layer1, layer]))\n            if batch_normalization:\n                layer1 = BatchNormalization()(layer1)\n            layer1 = Activation('relu')(layer1)\n        layer = layer1\n\n    out_layer = add([layer, skip_layer])\n    return out_layer\n\n########################################################################################################\n# Define the neural network\ndef unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format= data_format)(x)\n        features = features * 2\n\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    x = Dropout(0.2)(x)\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        # attention_up_and_concate(x,[skips[i])\n        x = UpSampling2D(size=(2, 2), data_format=data_format)(x)\n        x = concatenate([skips[i], x], axis=1)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n\n    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Attention U-Net\ndef att_unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format='channels_first')(x)\n        features = features * 2\n\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    x = Dropout(0.2)(x)\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n\n    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\ndef r2_unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = rec_res_block(x, features, data_format=data_format)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n\n        features = features * 2\n\n    x = rec_res_block(x, features, data_format=data_format)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = up_and_concate(x, skips[i], data_format=data_format)\n        x = rec_res_block(x, features, data_format=data_format)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n    #model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Attention R2U-Net\ndef att_r2_unet(img_w, img_h, n_label, data_format='channels_last'):\n    inputs = Input((img_w, img_h , 3))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = rec_res_block(x, features, data_format=data_format)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n\n        features = features * 2\n\n    x = rec_res_block(x, features, data_format=data_format)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n        x = rec_res_block(x, features, data_format=data_format)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n    #model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n    return model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T13:00:10.909316Z","iopub.execute_input":"2021-07-26T13:00:10.909668Z","iopub.status.idle":"2021-07-26T13:00:10.959420Z","shell.execute_reply.started":"2021-07-26T13:00:10.909639Z","shell.execute_reply":"2021-07-26T13:00:10.958329Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:10.962976Z","iopub.execute_input":"2021-07-26T13:00:10.963261Z","iopub.status.idle":"2021-07-26T13:00:10.969159Z","shell.execute_reply.started":"2021-07-26T13:00:10.963237Z","shell.execute_reply":"2021-07-26T13:00:10.968396Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nimport numpy as np\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nimport cv2\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nBackGround = [255, 255, 255]\nroad = [0, 0, 0]\n# COLOR_DICT = np.array([BackGround, road])\none = [128, 128, 128]\ntwo = [128, 0, 0]\nthree = [192, 192, 128]\nfour = [255, 69, 0]\nfive = [128, 64, 128]\nsix = [60, 40, 222]\nseven = [128, 128, 0]\neight = [192, 128, 128]\nnine = [64, 64, 128]\nten = [64, 0, 128]\neleven = [64, 64, 0]\ntwelve = [0, 128, 192]\nCOLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n\n\nclass data_preprocess:\n    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n                 test_path=None, save_path=None,\n                 img_rows=512, img_cols=512,\n                 flag_multi_class=False,\n                 num_classes = 2):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.train_path = train_path\n        self.image_folder = image_folder\n        self.label_folder = label_folder\n        self.valid_path = valid_path\n        self.valid_image_folder = valid_image_folder\n        self.valid_label_folder = valid_label_folder\n        self.test_path = test_path\n        self.save_path = save_path\n        self.data_gen_args = dict(rotation_range=50,\n                                  vertical_flip=True,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n        self.image_color_mode = \"rgb\"\n        self.label_color_mode = \"grayscale\"\n\n        self.flag_multi_class = flag_multi_class\n        self.num_class = num_classes\n        self.target_size = (img_cols, img_rows)\n        self.img_type = 'png'\n\n    def adjustData(self, img, label):\n        if (self.flag_multi_class):\n            img = img / 255.\n            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n            new_label = np.zeros(label.shape + (self.num_class,))\n            for i in range(self.num_class):\n                new_label[label == i, i] = 1\n            label = new_label\n        elif (np.max(img) > 1):\n            #img = img / 255.\n            #label = label / 255.\n            #label[label >= 0.5] = 1\n            #label[label < 0.5] = 0\n            img2 =np.asarray(img)\n            label2 =np.asarray(label)\n            img2 =img2.astype('float32')\n            label2 =label2.astype('float32')\n            img2 /= 255.0\n            label2 /= 255.0\n            label2[label2 >= 0.5] = 1\n            label2[label2 < 0.5] = 0\n        return (img2, label2)\n\n    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n                       save_to_dir=None, seed=7):\n        '''\n        can generate image and label at the same time\n        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n        '''\n        image_datagen = ImageDataGenerator(**self.data_gen_args)\n        label_datagen = ImageDataGenerator(**self.data_gen_args)\n        image_generator = image_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=image_save_prefix,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=label_save_prefix,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n\n    def testGenerator(self):#no es usado por el momento\n        filenames = os.listdir(self.test_path)\n        for filename in filenames:\n            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n            img = img / 255.\n            img = trans.resize(img, self.target_size, mode='constant')\n            img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n            img = np.reshape(img, (1,) + img.shape)\n            yield img\n\n    def validLoad(self, batch_size,seed=7):\n        image_datagen = ImageDataGenerator(rescale=0)\n        label_datagen = ImageDataGenerator(rescale=0)\n        image_generator = image_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)        \n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n        \n        # return imgs,labels\n\n    def saveResult(self, npyfile, size, name,threshold=80):#version alterna para guardar las predicciones\n        for i, item in enumerate(npyfile):\n            img = item\n            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n            if self.flag_multi_class:\n                for row in range(len(img)):\n                    for col in range(len(img[row])):\n                        num = np.argmax(img[row][col])\n                        img_std[row][col] = COLOR_DICT[num]\n            else:\n                for k in range(len(img)):\n                    for j in range(len(img[k])):\n                        num = img[k][j]\n                        if num < (threshold/255.0):\n                            img_std[k][j] = road\n                        else:\n                            img_std[k][j] = BackGround\n            img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:11.165704Z","iopub.execute_input":"2021-07-26T13:00:11.165982Z","iopub.status.idle":"2021-07-26T13:00:11.197605Z","shell.execute_reply.started":"2021-07-26T13:00:11.165957Z","shell.execute_reply":"2021-07-26T13:00:11.196628Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\nimport numpy as np\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\ndef iou_coeff(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n    mvalue=(intersection+smooth)/(union+smooth)\n    return mvalue\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n    Only computes a batch-wise average of precision.\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n        Only computes a batch-wise average of recall.\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef ACL5(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((512, 512))\n\tC_2 = np.zeros((512, 512))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n\n\treturn loss","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:11.199167Z","iopub.execute_input":"2021-07-26T13:00:11.199535Z","iopub.status.idle":"2021-07-26T13:00:11.216427Z","shell.execute_reply.started":"2021-07-26T13:00:11.199500Z","shell.execute_reply":"2021-07-26T13:00:11.215533Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport keras\nfrom keras.callbacks import TensorBoard\nimport tensorflow as tf\n#import keras.backend.tensorflow_backend as K\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import CSVLogger\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n    \n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth=True\n#sess = tf.Session(config=config)\n#K.set_session(sess)\n #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n\n#path to images which are prepared to train a model\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\nlr = 8.00E-05\nbatch = 2\n\n\n\nopt = tf.keras.optimizers.Adam(lr , beta_1=0.9,\n    beta_2=0.999)\n\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=2)\n#Number Of Batches\n#rain_steps = len(train_data)//batch\n#valid_steps = len(valid_data)//batch\n\n\n#if len(train_data) % batch != 0:\n#        train_steps += 1\n#if len(valid_data) % batch != 0:\n#        valid_steps += 1\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:11.218278Z","iopub.execute_input":"2021-07-26T13:00:11.218685Z","iopub.status.idle":"2021-07-26T13:00:11.259026Z","shell.execute_reply.started":"2021-07-26T13:00:11.218649Z","shell.execute_reply":"2021-07-26T13:00:11.258231Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"\nbeta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\ndef tversky_index( y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                1 - alpha) * false_pos + smooth)\n\ndef tversky_loss( y_true, y_pred):\n    return 1 - tversky_index(y_true, y_pred)\n\ndef focal_tversky( y_true, y_pred):\n    pt_1 = tversky_index(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1 - pt_1), gamma)\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:11.260363Z","iopub.execute_input":"2021-07-26T13:00:11.260666Z","iopub.status.idle":"2021-07-26T13:00:11.274248Z","shell.execute_reply.started":"2021-07-26T13:00:11.260639Z","shell.execute_reply":"2021-07-26T13:00:11.273499Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, add,average,concatenate\nfrom keras.layers.core import Lambda\nfrom keras.optimizers import *\nfrom keras.losses import binary_crossentropy\n\n\n\nIMG_SIZE = 512\nh_heuns_method=0.5\n\ndef res_block(x, nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n    return res_path\ndef res_block2(x,y,nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n\n    res_path = average([y, res_path])#suma doble \n    return res_path\n\n\ndef encoder(x):\n    to_decoder = []\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(main_path)\n\n    shortcut = Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1))(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    main_path = add([shortcut, hpath])#suma corta\n\n    to_decoder.append(main_path)\n\n\n    s1 = Conv2D(filters=128, kernel_size=(1, 1), strides=(2, 2))(x)\n    s1 = BatchNormalization()(s1)\n\n    main_path = res_block2(main_path,s1, [128, 128], [(2, 2), (1, 1)]) \n    to_decoder.append(main_path)\n\n    main_path = res_block(main_path, [256, 256], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    s2 = Conv2D(filters=512, kernel_size=(1, 1), strides=(4, 4))(to_decoder[1])\n    s2 = BatchNormalization()(s2)\n\n    main_path = res_block2(main_path,s2, [512, 512], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    return to_decoder\n\n\ndef decoder(x, from_encoder):\n    main_path = UpSampling2D(size=(2, 2))(x)#32x32\n    main_path1 = concatenate([main_path, from_encoder[3]], axis=3)\n    main_path = res_block(main_path1, [512, 512], [(1, 1), (1, 1)])\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)###64x64\n    main_path = concatenate([main_path, from_encoder[2]], axis=3)#\n    u1 = UpSampling2D(size=(2, 2))(main_path1)#\n    u1 = Conv2D(256, kernel_size=(1, 1),strides=(1, 1))(u1)\n    u1 = BatchNormalization()(u1)\n    main_path = res_block2(main_path,u1, [256, 256], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)#128x128\n    main_path2 = concatenate([main_path, from_encoder[1]], axis=3)#\n    main_path = res_block(main_path2, [128, 128], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)#256x256\n    main_path = concatenate([main_path, from_encoder[0]], axis=3)#256x256\n\n    u2 = UpSampling2D(size=(2,2))(main_path2)#\n    u2 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u2)#\n    u2 = BatchNormalization()(u2)\n    main_path = res_block2(main_path,u2, [64, 64], [(1, 1), (1, 1)])#256x256\n\n    return main_path\n\n\ndef res2unet(lrate=8.00E-05,pretrained_weights=None):\n    print(lrate)\n    input_size=(512, 512, 3)\n    inputs = Input(shape=input_size)\n\n    to_decoder = encoder(inputs)\n\n    path = res_block(to_decoder[3], [1024, 1024], [(2, 2), (1, 1)])####bridge\n\n    path = decoder(path, from_encoder=to_decoder)\n\n\n    path = Conv2D(2, kernel_size=(3, 3),activation='relu', padding='same', strides=(1, 1))(path)\n    path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path)\n    model = Model(inputs=inputs, outputs=path)\n    #model.compile(optimizer=Adam(lr=lrate), loss=ACL5, metrics=[dice_loss,iou_coeff,precision,recall])\n    #model.summary()\n    if (pretrained_weights):\n        model.load_weights(pretrained_weights)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:11.275498Z","iopub.execute_input":"2021-07-26T13:00:11.275955Z","iopub.status.idle":"2021-07-26T13:00:11.306192Z","shell.execute_reply.started":"2021-07-26T13:00:11.275918Z","shell.execute_reply":"2021-07-26T13:00:11.305316Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:11.307588Z","iopub.execute_input":"2021-07-26T13:00:11.308086Z","iopub.status.idle":"2021-07-26T13:00:11.318847Z","shell.execute_reply.started":"2021-07-26T13:00:11.308050Z","shell.execute_reply":"2021-07-26T13:00:11.318102Z"},"trusted":true},"execution_count":168,"outputs":[{"name":"stdout","text":"env: SM_FRAMEWORK=tf.keras\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install segmentation_models\nimport segmentation_models\nfrom segmentation_models.losses import bce_jaccard_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:11.319853Z","iopub.execute_input":"2021-07-26T13:00:11.320095Z","iopub.status.idle":"2021-07-26T13:00:17.261982Z","shell.execute_reply.started":"2021-07-26T13:00:11.320064Z","shell.execute_reply":"2021-07-26T13:00:17.261031Z"},"trusted":true},"execution_count":169,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models in /opt/conda/lib/python3.7/site-packages (1.0.1)\nRequirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.8)\nRequirement already satisfied: image-classifiers==1.0.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\nRequirement already satisfied: efficientnet==1.0.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation_models) (0.18.2)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.7.2)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.9.0)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.6.3)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (8.2.0)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.4.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (5.0.9)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"def ACL5_bce_jaccard_loss(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + 1.75*bce_jaccard_loss(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:17.263580Z","iopub.execute_input":"2021-07-26T13:00:17.263920Z","iopub.status.idle":"2021-07-26T13:00:17.270927Z","shell.execute_reply.started":"2021-07-26T13:00:17.263882Z","shell.execute_reply":"2021-07-26T13:00:17.270162Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimg_w  , img_h = 512 , 512\nn_label = 1\n#model =res2unet(lrate=8.00E-04,pretrained_weights=None)\n#model = unet(lrate=1e-4,ls=2)# WBCE\nmodel = att_r2_unet(img_w, img_h, n_label, data_format='channels_last')\nmetrics = [\"acc\" , recall , precision  , iou_coeff ,dice_loss]\nmodel_checkpoint1 = keras.callbacks.ModelCheckpoint('Res2Net.hdf5', monitor= 'val_iou_coeff' ,verbose=1,mode='min',save_best_only=True)#para guardar el entranamiento con menor dice loss en validation\n#steps_per_epochnumber= 600 / 2\n#validation_stepsnumber = 200 / 2\n\ncsv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')#respaldo de datos de entranamiento \nmodel.compile(optimizer= opt, loss=ACL5_bce_jaccard_loss , metrics=[\"acc\" , recall , precision , iou_coeff ,dice_loss])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:17.275257Z","iopub.execute_input":"2021-07-26T13:00:17.275550Z","iopub.status.idle":"2021-07-26T13:00:17.931906Z","shell.execute_reply.started":"2021-07-26T13:00:17.275525Z","shell.execute_reply":"2021-07-26T13:00:17.931057Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"8e-05\nModel: \"model_11\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_19 (InputLayer)           [(None, 512, 512, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_895 (Conv2D)             (None, 512, 512, 64) 1792        input_19[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_330 (BatchN (None, 512, 512, 64) 256         conv2d_895[0][0]                 \n__________________________________________________________________________________________________\nactivation_628 (Activation)     (None, 512, 512, 64) 0           batch_normalization_330[0][0]    \n__________________________________________________________________________________________________\nconv2d_897 (Conv2D)             (None, 512, 512, 64) 256         input_19[0][0]                   \n__________________________________________________________________________________________________\nconv2d_896 (Conv2D)             (None, 512, 512, 64) 36928       activation_628[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_331 (BatchN (None, 512, 512, 64) 256         conv2d_897[0][0]                 \n__________________________________________________________________________________________________\nlambda_127 (Lambda)             (None, 512, 512, 64) 0           conv2d_896[0][0]                 \n__________________________________________________________________________________________________\nadd_442 (Add)                   (None, 512, 512, 64) 0           batch_normalization_331[0][0]    \n                                                                 lambda_127[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_333 (BatchN (None, 512, 512, 64) 256         add_442[0][0]                    \n__________________________________________________________________________________________________\nactivation_629 (Activation)     (None, 512, 512, 64) 0           batch_normalization_333[0][0]    \n__________________________________________________________________________________________________\nconv2d_899 (Conv2D)             (None, 256, 256, 128 73856       activation_629[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_334 (BatchN (None, 256, 256, 128 512         conv2d_899[0][0]                 \n__________________________________________________________________________________________________\nactivation_630 (Activation)     (None, 256, 256, 128 0           batch_normalization_334[0][0]    \n__________________________________________________________________________________________________\nconv2d_901 (Conv2D)             (None, 256, 256, 128 8320        add_442[0][0]                    \n__________________________________________________________________________________________________\nconv2d_900 (Conv2D)             (None, 256, 256, 128 147584      activation_630[0][0]             \n__________________________________________________________________________________________________\nconv2d_898 (Conv2D)             (None, 256, 256, 128 512         input_19[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_335 (BatchN (None, 256, 256, 128 512         conv2d_901[0][0]                 \n__________________________________________________________________________________________________\nlambda_128 (Lambda)             (None, 256, 256, 128 0           conv2d_900[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_332 (BatchN (None, 256, 256, 128 512         conv2d_898[0][0]                 \n__________________________________________________________________________________________________\nadd_443 (Add)                   (None, 256, 256, 128 0           batch_normalization_335[0][0]    \n                                                                 lambda_128[0][0]                 \n__________________________________________________________________________________________________\naverage_44 (Average)            (None, 256, 256, 128 0           batch_normalization_332[0][0]    \n                                                                 add_443[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_336 (BatchN (None, 256, 256, 128 512         average_44[0][0]                 \n__________________________________________________________________________________________________\nactivation_631 (Activation)     (None, 256, 256, 128 0           batch_normalization_336[0][0]    \n__________________________________________________________________________________________________\nconv2d_902 (Conv2D)             (None, 128, 128, 256 295168      activation_631[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_337 (BatchN (None, 128, 128, 256 1024        conv2d_902[0][0]                 \n__________________________________________________________________________________________________\nactivation_632 (Activation)     (None, 128, 128, 256 0           batch_normalization_337[0][0]    \n__________________________________________________________________________________________________\nconv2d_904 (Conv2D)             (None, 128, 128, 256 33024       average_44[0][0]                 \n__________________________________________________________________________________________________\nconv2d_903 (Conv2D)             (None, 128, 128, 256 590080      activation_632[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_338 (BatchN (None, 128, 128, 256 1024        conv2d_904[0][0]                 \n__________________________________________________________________________________________________\nlambda_129 (Lambda)             (None, 128, 128, 256 0           conv2d_903[0][0]                 \n__________________________________________________________________________________________________\nadd_444 (Add)                   (None, 128, 128, 256 0           batch_normalization_338[0][0]    \n                                                                 lambda_129[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_340 (BatchN (None, 128, 128, 256 1024        add_444[0][0]                    \n__________________________________________________________________________________________________\nactivation_633 (Activation)     (None, 128, 128, 256 0           batch_normalization_340[0][0]    \n__________________________________________________________________________________________________\nconv2d_906 (Conv2D)             (None, 64, 64, 512)  1180160     activation_633[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_341 (BatchN (None, 64, 64, 512)  2048        conv2d_906[0][0]                 \n__________________________________________________________________________________________________\nactivation_634 (Activation)     (None, 64, 64, 512)  0           batch_normalization_341[0][0]    \n__________________________________________________________________________________________________\nconv2d_908 (Conv2D)             (None, 64, 64, 512)  131584      add_444[0][0]                    \n__________________________________________________________________________________________________\nconv2d_907 (Conv2D)             (None, 64, 64, 512)  2359808     activation_634[0][0]             \n__________________________________________________________________________________________________\nconv2d_905 (Conv2D)             (None, 64, 64, 512)  66048       average_44[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_342 (BatchN (None, 64, 64, 512)  2048        conv2d_908[0][0]                 \n__________________________________________________________________________________________________\nlambda_130 (Lambda)             (None, 64, 64, 512)  0           conv2d_907[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_339 (BatchN (None, 64, 64, 512)  2048        conv2d_905[0][0]                 \n__________________________________________________________________________________________________\nadd_445 (Add)                   (None, 64, 64, 512)  0           batch_normalization_342[0][0]    \n                                                                 lambda_130[0][0]                 \n__________________________________________________________________________________________________\naverage_45 (Average)            (None, 64, 64, 512)  0           batch_normalization_339[0][0]    \n                                                                 add_445[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_343 (BatchN (None, 64, 64, 512)  2048        average_45[0][0]                 \n__________________________________________________________________________________________________\nactivation_635 (Activation)     (None, 64, 64, 512)  0           batch_normalization_343[0][0]    \n__________________________________________________________________________________________________\nconv2d_909 (Conv2D)             (None, 32, 32, 1024) 4719616     activation_635[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_344 (BatchN (None, 32, 32, 1024) 4096        conv2d_909[0][0]                 \n__________________________________________________________________________________________________\nactivation_636 (Activation)     (None, 32, 32, 1024) 0           batch_normalization_344[0][0]    \n__________________________________________________________________________________________________\nconv2d_911 (Conv2D)             (None, 32, 32, 1024) 525312      average_45[0][0]                 \n__________________________________________________________________________________________________\nconv2d_910 (Conv2D)             (None, 32, 32, 1024) 9438208     activation_636[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_345 (BatchN (None, 32, 32, 1024) 4096        conv2d_911[0][0]                 \n__________________________________________________________________________________________________\nlambda_131 (Lambda)             (None, 32, 32, 1024) 0           conv2d_910[0][0]                 \n__________________________________________________________________________________________________\nadd_446 (Add)                   (None, 32, 32, 1024) 0           batch_normalization_345[0][0]    \n                                                                 lambda_131[0][0]                 \n__________________________________________________________________________________________________\nup_sampling2d_94 (UpSampling2D) (None, 64, 64, 1024) 0           add_446[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_44 (Concatenate)    (None, 64, 64, 1536) 0           up_sampling2d_94[0][0]           \n                                                                 average_45[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_346 (BatchN (None, 64, 64, 1536) 6144        concatenate_44[0][0]             \n__________________________________________________________________________________________________\nactivation_637 (Activation)     (None, 64, 64, 1536) 0           batch_normalization_346[0][0]    \n__________________________________________________________________________________________________\nconv2d_912 (Conv2D)             (None, 64, 64, 512)  7078400     activation_637[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_347 (BatchN (None, 64, 64, 512)  2048        conv2d_912[0][0]                 \n__________________________________________________________________________________________________\nactivation_638 (Activation)     (None, 64, 64, 512)  0           batch_normalization_347[0][0]    \n__________________________________________________________________________________________________\nconv2d_914 (Conv2D)             (None, 64, 64, 512)  786944      concatenate_44[0][0]             \n__________________________________________________________________________________________________\nconv2d_913 (Conv2D)             (None, 64, 64, 512)  2359808     activation_638[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_348 (BatchN (None, 64, 64, 512)  2048        conv2d_914[0][0]                 \n__________________________________________________________________________________________________\nlambda_132 (Lambda)             (None, 64, 64, 512)  0           conv2d_913[0][0]                 \n__________________________________________________________________________________________________\nadd_447 (Add)                   (None, 64, 64, 512)  0           batch_normalization_348[0][0]    \n                                                                 lambda_132[0][0]                 \n__________________________________________________________________________________________________\nup_sampling2d_95 (UpSampling2D) (None, 128, 128, 512 0           add_447[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_45 (Concatenate)    (None, 128, 128, 768 0           up_sampling2d_95[0][0]           \n                                                                 add_444[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_350 (BatchN (None, 128, 128, 768 3072        concatenate_45[0][0]             \n__________________________________________________________________________________________________\nactivation_639 (Activation)     (None, 128, 128, 768 0           batch_normalization_350[0][0]    \n__________________________________________________________________________________________________\nconv2d_916 (Conv2D)             (None, 128, 128, 256 1769728     activation_639[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_351 (BatchN (None, 128, 128, 256 1024        conv2d_916[0][0]                 \n__________________________________________________________________________________________________\nactivation_640 (Activation)     (None, 128, 128, 256 0           batch_normalization_351[0][0]    \n__________________________________________________________________________________________________\nup_sampling2d_96 (UpSampling2D) (None, 128, 128, 153 0           concatenate_44[0][0]             \n__________________________________________________________________________________________________\nconv2d_918 (Conv2D)             (None, 128, 128, 256 196864      concatenate_45[0][0]             \n__________________________________________________________________________________________________\nconv2d_917 (Conv2D)             (None, 128, 128, 256 590080      activation_640[0][0]             \n__________________________________________________________________________________________________\nconv2d_915 (Conv2D)             (None, 128, 128, 256 393472      up_sampling2d_96[0][0]           \n__________________________________________________________________________________________________\nbatch_normalization_352 (BatchN (None, 128, 128, 256 1024        conv2d_918[0][0]                 \n__________________________________________________________________________________________________\nlambda_133 (Lambda)             (None, 128, 128, 256 0           conv2d_917[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_349 (BatchN (None, 128, 128, 256 1024        conv2d_915[0][0]                 \n__________________________________________________________________________________________________\nadd_448 (Add)                   (None, 128, 128, 256 0           batch_normalization_352[0][0]    \n                                                                 lambda_133[0][0]                 \n__________________________________________________________________________________________________\naverage_46 (Average)            (None, 128, 128, 256 0           batch_normalization_349[0][0]    \n                                                                 add_448[0][0]                    \n__________________________________________________________________________________________________\nup_sampling2d_97 (UpSampling2D) (None, 256, 256, 256 0           average_46[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_46 (Concatenate)    (None, 256, 256, 384 0           up_sampling2d_97[0][0]           \n                                                                 average_44[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_353 (BatchN (None, 256, 256, 384 1536        concatenate_46[0][0]             \n__________________________________________________________________________________________________\nactivation_641 (Activation)     (None, 256, 256, 384 0           batch_normalization_353[0][0]    \n__________________________________________________________________________________________________\nconv2d_919 (Conv2D)             (None, 256, 256, 128 442496      activation_641[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_354 (BatchN (None, 256, 256, 128 512         conv2d_919[0][0]                 \n__________________________________________________________________________________________________\nactivation_642 (Activation)     (None, 256, 256, 128 0           batch_normalization_354[0][0]    \n__________________________________________________________________________________________________\nconv2d_921 (Conv2D)             (None, 256, 256, 128 49280       concatenate_46[0][0]             \n__________________________________________________________________________________________________\nconv2d_920 (Conv2D)             (None, 256, 256, 128 147584      activation_642[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_355 (BatchN (None, 256, 256, 128 512         conv2d_921[0][0]                 \n__________________________________________________________________________________________________\nlambda_134 (Lambda)             (None, 256, 256, 128 0           conv2d_920[0][0]                 \n__________________________________________________________________________________________________\nadd_449 (Add)                   (None, 256, 256, 128 0           batch_normalization_355[0][0]    \n                                                                 lambda_134[0][0]                 \n__________________________________________________________________________________________________\nup_sampling2d_98 (UpSampling2D) (None, 512, 512, 128 0           add_449[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_47 (Concatenate)    (None, 512, 512, 192 0           up_sampling2d_98[0][0]           \n                                                                 add_442[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_357 (BatchN (None, 512, 512, 192 768         concatenate_47[0][0]             \n__________________________________________________________________________________________________\nactivation_643 (Activation)     (None, 512, 512, 192 0           batch_normalization_357[0][0]    \n__________________________________________________________________________________________________\nconv2d_923 (Conv2D)             (None, 512, 512, 64) 110656      activation_643[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_358 (BatchN (None, 512, 512, 64) 256         conv2d_923[0][0]                 \n__________________________________________________________________________________________________\nactivation_644 (Activation)     (None, 512, 512, 64) 0           batch_normalization_358[0][0]    \n__________________________________________________________________________________________________\nup_sampling2d_99 (UpSampling2D) (None, 512, 512, 384 0           concatenate_46[0][0]             \n__________________________________________________________________________________________________\nconv2d_925 (Conv2D)             (None, 512, 512, 64) 12352       concatenate_47[0][0]             \n__________________________________________________________________________________________________\nconv2d_924 (Conv2D)             (None, 512, 512, 64) 36928       activation_644[0][0]             \n__________________________________________________________________________________________________\nconv2d_922 (Conv2D)             (None, 512, 512, 64) 24640       up_sampling2d_99[0][0]           \n__________________________________________________________________________________________________\nbatch_normalization_359 (BatchN (None, 512, 512, 64) 256         conv2d_925[0][0]                 \n__________________________________________________________________________________________________\nlambda_135 (Lambda)             (None, 512, 512, 64) 0           conv2d_924[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_356 (BatchN (None, 512, 512, 64) 256         conv2d_922[0][0]                 \n__________________________________________________________________________________________________\nadd_450 (Add)                   (None, 512, 512, 64) 0           batch_normalization_359[0][0]    \n                                                                 lambda_135[0][0]                 \n__________________________________________________________________________________________________\naverage_47 (Average)            (None, 512, 512, 64) 0           batch_normalization_356[0][0]    \n                                                                 add_450[0][0]                    \n__________________________________________________________________________________________________\nconv2d_926 (Conv2D)             (None, 512, 512, 2)  1154        average_47[0][0]                 \n__________________________________________________________________________________________________\nconv2d_927 (Conv2D)             (None, 512, 512, 1)  3           conv2d_926[0][0]                 \n==================================================================================================\nTotal params: 33,651,397\nTrainable params: 33,630,021\nNon-trainable params: 21,376\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit_generator(train_data,\n                              epochs=32,\n                              steps_per_epoch=300,\n                              validation_steps=100,\n                              validation_data=valid_data,\n                              callbacks=[model_checkpoint1 , ReduceLROnPlateau(monitor='iou_coeff', factor=0.1, patience=4)])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:17.933167Z","iopub.execute_input":"2021-07-26T13:00:17.933474Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Found 600 images belonging to 1 classes.\nFound 600 images belonging to 1 classes.\nEpoch 1/32\n(None, 512, 512, 1)\n(None, 512, 512, 1)\n300/300 [==============================] - ETA: 0s - loss: 317733.1215 - acc: 0.9666 - recall: 0.1235 - precision: 0.2332 - iou_coeff: 0.0911 - dice_loss: 0.8442Found 200 images belonging to 1 classes.\nFound 200 images belonging to 1 classes.\n(None, 512, 512, 1)\n300/300 [==============================] - 167s 543ms/step - loss: 317131.6531 - acc: 0.9667 - recall: 0.1238 - precision: 0.2339 - iou_coeff: 0.0913 - dice_loss: 0.8438 - val_loss: 39456.3164 - val_acc: 0.9867 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_iou_coeff: 0.0015 - val_dice_loss: 0.9971\n\nEpoch 00001: val_iou_coeff improved from inf to 0.00151, saving model to Res2Net.hdf5\nEpoch 2/32\n300/300 [==============================] - 161s 538ms/step - loss: 28265.5019 - acc: 0.9903 - recall: 0.4855 - precision: 0.6967 - iou_coeff: 0.3556 - dice_loss: 0.4924 - val_loss: 35176.2266 - val_acc: 0.9869 - val_recall: 0.0062 - val_precision: 0.0861 - val_iou_coeff: 0.0056 - val_dice_loss: 0.9900\n\nEpoch 00002: val_iou_coeff did not improve from 0.00151\nEpoch 3/32\n300/300 [==============================] - 161s 537ms/step - loss: 24692.2850 - acc: 0.9911 - recall: 0.5244 - precision: 0.7386 - iou_coeff: 0.4266 - dice_loss: 0.4226 - val_loss: 30318.2500 - val_acc: 0.9890 - val_recall: 0.5596 - val_precision: 0.6084 - val_iou_coeff: 0.3894 - val_dice_loss: 0.4596\n\nEpoch 00003: val_iou_coeff did not improve from 0.00151\nEpoch 4/32\n300/300 [==============================] - 161s 538ms/step - loss: 22930.9623 - acc: 0.9917 - recall: 0.5364 - precision: 0.7699 - iou_coeff: 0.4519 - dice_loss: 0.3932 - val_loss: 28269.6777 - val_acc: 0.9895 - val_recall: 0.7294 - val_precision: 0.6237 - val_iou_coeff: 0.4960 - val_dice_loss: 0.3493\n\nEpoch 00004: val_iou_coeff did not improve from 0.00151\nEpoch 5/32\n300/300 [==============================] - 161s 538ms/step - loss: 21376.5456 - acc: 0.9922 - recall: 0.5616 - precision: 0.7658 - iou_coeff: 0.4684 - dice_loss: 0.3782 - val_loss: 23931.4570 - val_acc: 0.9912 - val_recall: 0.5485 - val_precision: 0.7309 - val_iou_coeff: 0.4353 - val_dice_loss: 0.4182\n\nEpoch 00005: val_iou_coeff did not improve from 0.00151\nEpoch 6/32\n300/300 [==============================] - 161s 538ms/step - loss: 19730.0976 - acc: 0.9928 - recall: 0.6245 - precision: 0.7748 - iou_coeff: 0.5203 - dice_loss: 0.3292 - val_loss: 21231.3652 - val_acc: 0.9923 - val_recall: 0.6661 - val_precision: 0.7538 - val_iou_coeff: 0.5329 - val_dice_loss: 0.3181\n\nEpoch 00006: val_iou_coeff did not improve from 0.00151\nEpoch 7/32\n300/300 [==============================] - 161s 538ms/step - loss: 19168.1370 - acc: 0.9931 - recall: 0.6500 - precision: 0.7919 - iou_coeff: 0.5486 - dice_loss: 0.3030 - val_loss: 19822.7051 - val_acc: 0.9928 - val_recall: 0.5785 - val_precision: 0.8247 - val_iou_coeff: 0.5068 - val_dice_loss: 0.3467\n\nEpoch 00007: val_iou_coeff did not improve from 0.00151\nEpoch 8/32\n300/300 [==============================] - 161s 538ms/step - loss: 19835.5238 - acc: 0.9928 - recall: 0.6193 - precision: 0.8012 - iou_coeff: 0.5308 - dice_loss: 0.3203 - val_loss: 19422.1719 - val_acc: 0.9930 - val_recall: 0.5569 - val_precision: 0.8525 - val_iou_coeff: 0.5015 - val_dice_loss: 0.3525\n\nEpoch 00008: val_iou_coeff did not improve from 0.00151\nEpoch 9/32\n300/300 [==============================] - 161s 537ms/step - loss: 18420.2778 - acc: 0.9933 - recall: 0.6504 - precision: 0.8112 - iou_coeff: 0.5646 - dice_loss: 0.2924 - val_loss: 19013.1953 - val_acc: 0.9932 - val_recall: 0.6534 - val_precision: 0.8092 - val_iou_coeff: 0.5549 - val_dice_loss: 0.2995\n\nEpoch 00009: val_iou_coeff did not improve from 0.00151\nEpoch 10/32\n300/300 [==============================] - 161s 538ms/step - loss: 18155.5367 - acc: 0.9935 - recall: 0.6651 - precision: 0.8163 - iou_coeff: 0.5692 - dice_loss: 0.2862 - val_loss: 19199.3594 - val_acc: 0.9931 - val_recall: 0.6663 - val_precision: 0.7859 - val_iou_coeff: 0.5502 - val_dice_loss: 0.3032\n\nEpoch 00010: val_iou_coeff did not improve from 0.00151\nEpoch 11/32\n300/300 [==============================] - 161s 538ms/step - loss: 17066.6137 - acc: 0.9938 - recall: 0.6754 - precision: 0.8207 - iou_coeff: 0.5835 - dice_loss: 0.2739 - val_loss: 18608.8984 - val_acc: 0.9933 - val_recall: 0.6464 - val_precision: 0.8075 - val_iou_coeff: 0.5545 - val_dice_loss: 0.3045\n\nEpoch 00011: val_iou_coeff did not improve from 0.00151\nEpoch 12/32\n300/300 [==============================] - 161s 538ms/step - loss: 17201.8421 - acc: 0.9938 - recall: 0.6705 - precision: 0.8145 - iou_coeff: 0.5788 - dice_loss: 0.2787 - val_loss: 18728.2520 - val_acc: 0.9932 - val_recall: 0.6405 - val_precision: 0.8311 - val_iou_coeff: 0.5498 - val_dice_loss: 0.3027\n\nEpoch 00012: val_iou_coeff did not improve from 0.00151\nEpoch 13/32\n300/300 [==============================] - 161s 538ms/step - loss: 17800.5820 - acc: 0.9936 - recall: 0.6787 - precision: 0.8208 - iou_coeff: 0.5850 - dice_loss: 0.2733 - val_loss: 18934.4492 - val_acc: 0.9932 - val_recall: 0.6380 - val_precision: 0.8276 - val_iou_coeff: 0.5525 - val_dice_loss: 0.3018\n\nEpoch 00013: val_iou_coeff did not improve from 0.00151\nEpoch 14/32\n300/300 [==============================] - 161s 538ms/step - loss: 18921.5803 - acc: 0.9931 - recall: 0.6243 - precision: 0.8039 - iou_coeff: 0.5450 - dice_loss: 0.3129 - val_loss: 18571.2344 - val_acc: 0.9933 - val_recall: 0.6444 - val_precision: 0.8253 - val_iou_coeff: 0.5515 - val_dice_loss: 0.3021\n\nEpoch 00014: val_iou_coeff did not improve from 0.00151\nEpoch 15/32\n300/300 [==============================] - 161s 538ms/step - loss: 18234.0070 - acc: 0.9934 - recall: 0.6485 - precision: 0.8322 - iou_coeff: 0.5690 - dice_loss: 0.2875 - val_loss: 19185.2188 - val_acc: 0.9931 - val_recall: 0.6340 - val_precision: 0.8008 - val_iou_coeff: 0.5420 - val_dice_loss: 0.3126\n\nEpoch 00015: val_iou_coeff did not improve from 0.00151\nEpoch 16/32\n300/300 [==============================] - 161s 538ms/step - loss: 17569.0858 - acc: 0.9937 - recall: 0.6683 - precision: 0.8190 - iou_coeff: 0.5811 - dice_loss: 0.2777 - val_loss: 18308.2754 - val_acc: 0.9934 - val_recall: 0.6737 - val_precision: 0.8190 - val_iou_coeff: 0.5731 - val_dice_loss: 0.2836\n\nEpoch 00016: val_iou_coeff did not improve from 0.00151\nEpoch 17/32\n300/300 [==============================] - 161s 538ms/step - loss: 17730.0467 - acc: 0.9936 - recall: 0.6647 - precision: 0.8112 - iou_coeff: 0.5730 - dice_loss: 0.2844 - val_loss: 18284.8672 - val_acc: 0.9934 - val_recall: 0.6531 - val_precision: 0.8227 - val_iou_coeff: 0.5583 - val_dice_loss: 0.2942\n\nEpoch 00017: val_iou_coeff did not improve from 0.00151\nEpoch 18/32\n300/300 [==============================] - 161s 538ms/step - loss: 17945.4256 - acc: 0.9935 - recall: 0.6651 - precision: 0.8272 - iou_coeff: 0.5783 - dice_loss: 0.2776 - val_loss: 19176.0781 - val_acc: 0.9931 - val_recall: 0.6503 - val_precision: 0.8024 - val_iou_coeff: 0.5519 - val_dice_loss: 0.3069\n\nEpoch 00018: val_iou_coeff did not improve from 0.00151\nEpoch 19/32\n300/300 [==============================] - 161s 538ms/step - loss: 17992.6435 - acc: 0.9935 - recall: 0.6497 - precision: 0.8196 - iou_coeff: 0.5633 - dice_loss: 0.2940 - val_loss: 18242.4570 - val_acc: 0.9934 - val_recall: 0.6481 - val_precision: 0.8191 - val_iou_coeff: 0.5507 - val_dice_loss: 0.3010\n\nEpoch 00019: val_iou_coeff did not improve from 0.00151\nEpoch 20/32\n300/300 [==============================] - 162s 539ms/step - loss: 17362.9064 - acc: 0.9937 - recall: 0.6731 - precision: 0.8310 - iou_coeff: 0.5868 - dice_loss: 0.2724 - val_loss: 19195.8477 - val_acc: 0.9931 - val_recall: 0.6454 - val_precision: 0.8156 - val_iou_coeff: 0.5501 - val_dice_loss: 0.3014\n\nEpoch 00020: val_iou_coeff did not improve from 0.00151\nEpoch 21/32\n300/300 [==============================] - 161s 538ms/step - loss: 17182.2318 - acc: 0.9938 - recall: 0.6717 - precision: 0.8105 - iou_coeff: 0.5756 - dice_loss: 0.2827 - val_loss: 18834.0059 - val_acc: 0.9932 - val_recall: 0.6595 - val_precision: 0.8021 - val_iou_coeff: 0.5513 - val_dice_loss: 0.3037\n\nEpoch 00021: val_iou_coeff did not improve from 0.00151\nEpoch 22/32\n300/300 [==============================] - 161s 538ms/step - loss: 17842.3664 - acc: 0.9936 - recall: 0.6546 - precision: 0.8194 - iou_coeff: 0.5681 - dice_loss: 0.2884 - val_loss: 17869.8281 - val_acc: 0.9936 - val_recall: 0.6544 - val_precision: 0.8374 - val_iou_coeff: 0.5709 - val_dice_loss: 0.2848\n\nEpoch 00022: val_iou_coeff did not improve from 0.00151\nEpoch 23/32\n300/300 [==============================] - 161s 537ms/step - loss: 18403.0914 - acc: 0.9933 - recall: 0.6540 - precision: 0.8036 - iou_coeff: 0.5594 - dice_loss: 0.2954 - val_loss: 18936.7793 - val_acc: 0.9932 - val_recall: 0.6566 - val_precision: 0.8096 - val_iou_coeff: 0.5523 - val_dice_loss: 0.3028\n\nEpoch 00023: val_iou_coeff did not improve from 0.00151\nEpoch 24/32\n223/300 [=====================>........] - ETA: 37s - loss: 18893.8109 - acc: 0.9931 - recall: 0.6538 - precision: 0.8088 - iou_coeff: 0.5606 - dice_loss: 0.2942","output_type":"stream"}]},{"cell_type":"code","source":"test_path = '../input/testdata/Test'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + 'test_image_folder'\ntest_label_folder =\"label\"\n\n\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes , test_path = Test_path , save_path = './' )\n\ntest_data = dp.testGenerator()\n\nimport os\n  \n# Directory\ndirectory = \"RESULT_R2NET\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef read_image(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (512, 512))\n    x = x/255.0\n    return x\n\ndef read_mask(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, ( 512 , 512))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\n\n\n\ndef tf_parse(imagepath , maskpath):\n  def _parse(imagepath , maskpath):\n    x = read_image(imagepath)\n    y = read_mask(maskpath)\n\n    return x , y\n\n  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n  x.set_shape([512 , 512 , 3])\n  y.set_shape([512, 512, 1])\n\n  return x , y \n\n\ndef tf_dataset( imagepath , maskpath , batch = 2):\n  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n  dataset = dataset.map(tf_parse)\n  dataset = dataset.batch(batch)\n  dataset = dataset.repeat()\n  return dataset\n\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n\n    test_steps = (len(test_x)//batch_size)\n    if len(test_x) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n\n    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n        x = read_image(x)\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_DenseNet210WithDropout/{i}.png\", image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}