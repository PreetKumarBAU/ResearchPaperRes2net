{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\nimport keras.backend as K\n\n\n\n\ndef up_and_concate(down_layer, layer, data_format='channels_first'):\n    if data_format == 'channels_first':\n        in_channel = down_layer.get_shape().as_list()[1]\n    else:\n        in_channel = down_layer.get_shape().as_list()[3]\n\n    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n\n    if data_format == 'channels_first':\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n    else:\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n\n    concate = my_concat([up, layer])\n\n    return concate\n\n\ndef attention_up_and_concate(down_layer, layer, data_format='channels_first'):\n    if data_format == 'channels_first':\n        in_channel = down_layer.get_shape().as_list()[1]\n    else:\n        in_channel = down_layer.get_shape().as_list()[3]\n\n    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n\n    layer = attention_block_2d(x=layer, g=up, inter_channel=in_channel // 4, data_format=data_format)\n\n    if data_format == 'channels_first':\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n    else:\n        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n\n    concate = my_concat([up, layer])\n    return concate\n\n\ndef attention_block_2d(x, g, inter_channel, data_format='channels_first'):\n    # theta_x(?,g_height,g_width,inter_channel)\n\n    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(x)\n\n    # phi_g(?,g_height,g_width,inter_channel)\n\n    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(g)\n\n    # f(?,g_height,g_width,inter_channel)\n\n    f = Activation('relu')(add([theta_x, phi_g]))\n\n    # psi_f(?,g_height,g_width,1)\n\n    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format=data_format)(f)\n\n    rate = Activation('sigmoid')(psi_f)\n\n    # rate(?,x_height,x_width)\n\n    # att_x(?,x_height,x_width,x_channel)\n\n    att_x = multiply([x, rate])\n\n    return att_x\n\n\ndef res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n\n              padding='same', data_format='channels_first'):\n    if data_format == 'channels_first':\n        input_n_filters = input_layer.get_shape().as_list()[1]\n    else:\n        input_n_filters = input_layer.get_shape().as_list()[3]\n\n    layer = input_layer\n    for i in range(2):\n        layer = Conv2D(out_n_filters // 4, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n        if batch_normalization:\n            layer = BatchNormalization()(layer)\n        layer = Activation('relu')(layer)\n        layer = Conv2D(out_n_filters // 4, kernel_size, strides=stride, padding=padding, data_format=data_format)(layer)\n        layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(layer)\n\n    if out_n_filters != input_n_filters:\n        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n            input_layer)\n    else:\n        skip_layer = input_layer\n    out_layer = add([layer, skip_layer])\n    return out_layer\n\n\n# Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\ndef rec_res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n\n                  padding='same', data_format='channels_first'):\n    if data_format == 'channels_first':\n        input_n_filters = input_layer.get_shape().as_list()[1]\n    else:\n        input_n_filters = input_layer.get_shape().as_list()[3]\n\n    if out_n_filters != input_n_filters:\n        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n            input_layer)\n    else:\n        skip_layer = input_layer\n\n    layer = skip_layer\n    for j in range(2):\n\n        for i in range(2):\n            if i == 0:\n\n                layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n                    layer)\n                if batch_normalization:\n                    layer1 = BatchNormalization()(layer1)\n                layer1 = Activation('relu')(layer1)\n            layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n                add([layer1, layer]))\n            if batch_normalization:\n                layer1 = BatchNormalization()(layer1)\n            layer1 = Activation('relu')(layer1)\n        layer = layer1\n\n    out_layer = add([layer, skip_layer])\n    return out_layer\n\n########################################################################################################\n# Define the neural network\ndef unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format= data_format)(x)\n        features = features * 2\n\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    x = Dropout(0.2)(x)\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        # attention_up_and_concate(x,[skips[i])\n        x = UpSampling2D(size=(2, 2), data_format=data_format)(x)\n        x = concatenate([skips[i], x], axis=1)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n\n    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Attention U-Net\ndef att_unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format='channels_first')(x)\n        features = features * 2\n\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n    x = Dropout(0.2)(x)\n    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n        x = Dropout(0.2)(x)\n        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n\n    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\ndef r2_unet(img_w, img_h, n_label, data_format='channels_first'):\n    inputs = Input((3, img_w, img_h))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = rec_res_block(x, features, data_format=data_format)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n\n        features = features * 2\n\n    x = rec_res_block(x, features, data_format=data_format)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = up_and_concate(x, skips[i], data_format=data_format)\n        x = rec_res_block(x, features, data_format=data_format)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n    #model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n    return model\n\n\n########################################################################################################\n#Attention R2U-Net\ndef att_r2_unet(img_w, img_h, n_label, data_format='channels_last'):\n    inputs = Input((img_w, img_h , 3))\n    x = inputs\n    depth = 4\n    features = 64\n    skips = []\n    for i in range(depth):\n        x = rec_res_block(x, features, data_format=data_format)\n        skips.append(x)\n        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n\n        features = features * 2\n\n    x = rec_res_block(x, features, data_format=data_format)\n\n    for i in reversed(range(depth)):\n        features = features // 2\n        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n        x = rec_res_block(x, features, data_format=data_format)\n\n    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n    conv7 = core.Activation('sigmoid')(conv6)\n    model = Model(inputs=inputs, outputs=conv7)\n    #model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n    return model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T17:36:57.144493Z","iopub.execute_input":"2021-07-26T17:36:57.144881Z","iopub.status.idle":"2021-07-26T17:37:01.394017Z","shell.execute_reply.started":"2021-07-26T17:36:57.144798Z","shell.execute_reply":"2021-07-26T17:37:01.393175Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:01.395429Z","iopub.execute_input":"2021-07-26T17:37:01.395762Z","iopub.status.idle":"2021-07-26T17:37:02.097620Z","shell.execute_reply.started":"2021-07-26T17:37:01.395728Z","shell.execute_reply":"2021-07-26T17:37:02.096762Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nimport numpy as np\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nimport cv2\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nBackGround = [255, 255, 255]\nroad = [0, 0, 0]\n# COLOR_DICT = np.array([BackGround, road])\none = [128, 128, 128]\ntwo = [128, 0, 0]\nthree = [192, 192, 128]\nfour = [255, 69, 0]\nfive = [128, 64, 128]\nsix = [60, 40, 222]\nseven = [128, 128, 0]\neight = [192, 128, 128]\nnine = [64, 64, 128]\nten = [64, 0, 128]\neleven = [64, 64, 0]\ntwelve = [0, 128, 192]\nCOLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n\n\nclass data_preprocess:\n    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n                 test_path=None, save_path=None,\n                 img_rows=512, img_cols=512,\n                 flag_multi_class=False,\n                 num_classes = 2):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.train_path = train_path\n        self.image_folder = image_folder\n        self.label_folder = label_folder\n        self.valid_path = valid_path\n        self.valid_image_folder = valid_image_folder\n        self.valid_label_folder = valid_label_folder\n        self.test_path = test_path\n        self.save_path = save_path\n        self.data_gen_args = dict(rotation_range=50,\n                                  vertical_flip=True,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n        self.image_color_mode = \"rgb\"\n        self.label_color_mode = \"grayscale\"\n\n        self.flag_multi_class = flag_multi_class\n        self.num_class = num_classes\n        self.target_size = (img_cols, img_rows)\n        self.img_type = 'png'\n\n    def adjustData(self, img, label):\n        if (self.flag_multi_class):\n            img = img / 255.\n            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n            new_label = np.zeros(label.shape + (self.num_class,))\n            for i in range(self.num_class):\n                new_label[label == i, i] = 1\n            label = new_label\n        elif (np.max(img) > 1):\n            #img = img / 255.\n            #label = label / 255.\n            #label[label >= 0.5] = 1\n            #label[label < 0.5] = 0\n            img2 =np.asarray(img)\n            label2 =np.asarray(label)\n            img2 =img2.astype('float32')\n            label2 =label2.astype('float32')\n            img2 /= 255.0\n            label2 /= 255.0\n            label2[label2 >= 0.5] = 1\n            label2[label2 < 0.5] = 0\n        return (img2, label2)\n\n    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n                       save_to_dir=None, seed=7):\n        '''\n        can generate image and label at the same time\n        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n        '''\n        image_datagen = ImageDataGenerator(**self.data_gen_args)\n        label_datagen = ImageDataGenerator(**self.data_gen_args)\n        image_generator = image_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=image_save_prefix,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=label_save_prefix,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n\n    def testGenerator(self):#no es usado por el momento\n        filenames = os.listdir(self.test_path)\n        for filename in filenames:\n            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n            img = img / 255.\n            img = trans.resize(img, self.target_size, mode='constant')\n            img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n            img = np.reshape(img, (1,) + img.shape)\n            yield img\n\n    def validLoad(self, batch_size,seed=7):\n        image_datagen = ImageDataGenerator(rescale=0)\n        label_datagen = ImageDataGenerator(rescale=0)\n        image_generator = image_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)        \n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n        \n        # return imgs,labels\n\n    def saveResult(self, npyfile, size, name,threshold=80):#version alterna para guardar las predicciones\n        for i, item in enumerate(npyfile):\n            img = item\n            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n            if self.flag_multi_class:\n                for row in range(len(img)):\n                    for col in range(len(img[row])):\n                        num = np.argmax(img[row][col])\n                        img_std[row][col] = COLOR_DICT[num]\n            else:\n                for k in range(len(img)):\n                    for j in range(len(img[k])):\n                        num = img[k][j]\n                        if num < (threshold/255.0):\n                            img_std[k][j] = road\n                        else:\n                            img_std[k][j] = BackGround\n            img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:02.101352Z","iopub.execute_input":"2021-07-26T17:37:02.101657Z","iopub.status.idle":"2021-07-26T17:37:02.845472Z","shell.execute_reply.started":"2021-07-26T17:37:02.101632Z","shell.execute_reply":"2021-07-26T17:37:02.844263Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\nimport numpy as np\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\ndef iou_coeff(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n    mvalue=(intersection+smooth)/(union+smooth)\n    return mvalue\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n    Only computes a batch-wise average of precision.\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n        Only computes a batch-wise average of recall.\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef ACL5(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((512, 512))\n\tC_2 = np.zeros((512, 512))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n\n\treturn loss","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:02.850154Z","iopub.execute_input":"2021-07-26T17:37:02.850537Z","iopub.status.idle":"2021-07-26T17:37:02.884915Z","shell.execute_reply.started":"2021-07-26T17:37:02.850483Z","shell.execute_reply":"2021-07-26T17:37:02.879077Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport keras\nfrom keras.callbacks import TensorBoard\nimport tensorflow as tf\n#import keras.backend.tensorflow_backend as K\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import CSVLogger\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n    \n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth=True\n#sess = tf.Session(config=config)\n#K.set_session(sess)\n #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n\n#path to images which are prepared to train a model\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\nlr = 8.00E-05\nbatch = 2\n\n\n\nopt = tf.keras.optimizers.Adam(lr , beta_1=0.9,\n    beta_2=0.999)\n\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=2)\n#Number Of Batches\n#rain_steps = len(train_data)//batch\n#valid_steps = len(valid_data)//batch\n\n\n#if len(train_data) % batch != 0:\n#        train_steps += 1\n#if len(valid_data) % batch != 0:\n#        valid_steps += 1\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:02.889439Z","iopub.execute_input":"2021-07-26T17:37:02.889798Z","iopub.status.idle":"2021-07-26T17:37:02.910416Z","shell.execute_reply.started":"2021-07-26T17:37:02.889762Z","shell.execute_reply":"2021-07-26T17:37:02.909327Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\nbeta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\ndef tversky_index( y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                1 - alpha) * false_pos + smooth)\n\ndef tversky_loss( y_true, y_pred):\n    return 1 - tversky_index(y_true, y_pred)\n\ndef focal_tversky( y_true, y_pred):\n    pt_1 = tversky_index(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1 - pt_1), gamma)\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:02.911954Z","iopub.execute_input":"2021-07-26T17:37:02.912327Z","iopub.status.idle":"2021-07-26T17:37:02.939141Z","shell.execute_reply.started":"2021-07-26T17:37:02.912290Z","shell.execute_reply":"2021-07-26T17:37:02.937139Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, add,average,concatenate\nfrom keras.layers.core import Lambda\nfrom keras.optimizers import *\nfrom keras.losses import binary_crossentropy\n\n\n\nIMG_SIZE = 512\nh_heuns_method=0.5\n\ndef res_block(x, nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n    return res_path\ndef res_block2(x,y,nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n\n    res_path = average([y, res_path])#suma doble \n    return res_path\n\n\ndef encoder(x):\n    to_decoder = []\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(main_path)\n\n    shortcut = Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1))(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    main_path = add([shortcut, hpath])#suma corta\n\n    to_decoder.append(main_path)\n\n\n    s1 = Conv2D(filters=128, kernel_size=(1, 1), strides=(2, 2))(x)\n    s1 = BatchNormalization()(s1)\n\n    main_path = res_block2(main_path,s1, [128, 128], [(2, 2), (1, 1)]) \n    to_decoder.append(main_path)\n\n    main_path = res_block(main_path, [256, 256], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    s2 = Conv2D(filters=512, kernel_size=(1, 1), strides=(4, 4))(to_decoder[1])\n    s2 = BatchNormalization()(s2)\n\n    main_path = res_block2(main_path,s2, [512, 512], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    return to_decoder\n\n\ndef decoder(x, from_encoder):\n    main_path = UpSampling2D(size=(2, 2))(x)#32x32\n    main_path1 = concatenate([main_path, from_encoder[3]], axis=3)\n    main_path = res_block(main_path1, [512, 512], [(1, 1), (1, 1)])\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)###64x64\n    main_path = concatenate([main_path, from_encoder[2]], axis=3)#\n    u1 = UpSampling2D(size=(2, 2))(main_path1)#\n    u1 = Conv2D(256, kernel_size=(1, 1),strides=(1, 1))(u1)\n    u1 = BatchNormalization()(u1)\n    main_path = res_block2(main_path,u1, [256, 256], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)#128x128\n    main_path2 = concatenate([main_path, from_encoder[1]], axis=3)#\n    main_path = res_block(main_path2, [128, 128], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2))(main_path)#256x256\n    main_path = concatenate([main_path, from_encoder[0]], axis=3)#256x256\n\n    u2 = UpSampling2D(size=(2,2))(main_path2)#\n    u2 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u2)#\n    u2 = BatchNormalization()(u2)\n    main_path = res_block2(main_path,u2, [64, 64], [(1, 1), (1, 1)])#256x256\n\n    return main_path\n\n\ndef res2unet(lrate=8.00E-05,pretrained_weights=None):\n    print(lrate)\n    input_size=(512, 512, 3)\n    inputs = Input(shape=input_size)\n\n    to_decoder = encoder(inputs)\n\n    path = res_block(to_decoder[3], [1024, 1024], [(2, 2), (1, 1)])####bridge\n\n    path = decoder(path, from_encoder=to_decoder)\n\n\n    path = Conv2D(2, kernel_size=(3, 3),activation='relu', padding='same', strides=(1, 1))(path)\n    path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path)\n    model = Model(inputs=inputs, outputs=path)\n    #model.compile(optimizer=Adam(lr=lrate), loss=ACL5, metrics=[dice_loss,iou_coeff,precision,recall])\n    #model.summary()\n    if (pretrained_weights):\n        model.load_weights(pretrained_weights)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:02.949348Z","iopub.execute_input":"2021-07-26T17:37:02.949925Z","iopub.status.idle":"2021-07-26T17:37:03.047414Z","shell.execute_reply.started":"2021-07-26T17:37:02.949885Z","shell.execute_reply":"2021-07-26T17:37:03.043750Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:03.049527Z","iopub.execute_input":"2021-07-26T17:37:03.049851Z","iopub.status.idle":"2021-07-26T17:37:03.081381Z","shell.execute_reply.started":"2021-07-26T17:37:03.049819Z","shell.execute_reply":"2021-07-26T17:37:03.066016Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"env: SM_FRAMEWORK=tf.keras\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install segmentation_models\nimport segmentation_models\nfrom segmentation_models.losses import bce_jaccard_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:03.082686Z","iopub.execute_input":"2021-07-26T17:37:03.082986Z","iopub.status.idle":"2021-07-26T17:37:12.394500Z","shell.execute_reply.started":"2021-07-26T17:37:03.082956Z","shell.execute_reply":"2021-07-26T17:37:12.393546Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting segmentation_models\n  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\nCollecting keras-applications<=1.0.8,>=1.0.7\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 1.4 MB/s eta 0:00:01\n\u001b[?25hCollecting efficientnet==1.0.0\n  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\nCollecting image-classifiers==1.0.0\n  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation_models) (0.18.2)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (8.2.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.7.2)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.4.2)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.9.0)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.6.3)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (5.0.9)\nInstalling collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\nSuccessfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nSegmentation Models: using `tf.keras` framework.\n","output_type":"stream"}]},{"cell_type":"code","source":"def ACL5_bce_jaccard_loss(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + 1.5*bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n\ndef focal_tversky_bce_jaccard_loss(y_true, y_pred):\n    loss = focal_tversky(y_true, y_pred) + 2*bce_jaccard_loss(y_true, y_pred)\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:12.397777Z","iopub.execute_input":"2021-07-26T17:37:12.398102Z","iopub.status.idle":"2021-07-26T17:37:12.403482Z","shell.execute_reply.started":"2021-07-26T17:37:12.398044Z","shell.execute_reply":"2021-07-26T17:37:12.402404Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, add,average,concatenate\nfrom tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\n\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications import InceptionResNetV2\ndef squeeze_excite_block(inputs, ratio=8):\n    init = inputs       ## (b, 128, 128, 32)\n    channel_axis = -1\n    filters = init.shape[channel_axis]\n    se_shape = (1, 1, filters)\n\n    se = GlobalAveragePooling2D()(init)     ## (b, 32)   -> (b, 1, 1, 32)\n    se = Reshape(se_shape)(se)\n    se = Dense(filters//ratio, activation=\"relu\", use_bias=False)(se)\n    se = Dense(filters, activation=\"sigmoid\", use_bias=False)(se)\n\n    x = Multiply()([inputs, se])\n    return x\n\ndef ASPP(x, filter):\n    shape = x.shape\n\n    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n    y1 = BatchNormalization()(y1)\n    y1 = Activation(\"relu\")(y1)\n    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n\n    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n    y2 = BatchNormalization()(y2)\n    y2 = Activation(\"relu\")(y2)\n\n    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n    y3 = BatchNormalization()(y3)\n    y3 = Activation(\"relu\")(y3)\n\n    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n    y4 = BatchNormalization()(y4)\n    y4 = Activation(\"relu\")(y4)\n\n    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n    y5 = BatchNormalization()(y5)\n    y5 = Activation(\"relu\")(y5)\n\n    y = Concatenate()([y1, y2, y3, y4, y5])\n\n    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n    y = BatchNormalization()(y)\n    y = Activation(\"relu\")(y)\n\n    return y\n\ndef conv_block(x, filters):\n    x = Conv2D(filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = squeeze_excite_block(x)\n\n    return x\n\ndef encoder1(inputs):\n    skip_connections = []\n    model = InceptionResNetV2(include_top = False  , weights = \"imagenet\" , input_tensor = inputs)\n    #Skip Connection\n    s1 = model.get_layer(\"input_1\").output                 #Shape will be 256 * 256\n\n    s2 = model.get_layer(\"activation_2\").output         # 125 * 125 \n    s2 = ZeroPadding2D( ((2 , 1) ,  ( 2 , 1)) )(s2)          # 128 * 128\n\n\n    s3 = model.get_layer(\"activation_4\").output        # 60 * 60\n    s3 = ZeroPadding2D( ((2 , 2 ) , (2, 2)) )(s3)         # 64 * 64 \n\n\n    s4 = model.get_layer(\"activation_74\").output                              # 29*29\n    s4 = ZeroPadding2D( ((2 , 1) ,  ( 2 , 1)) )(s4) \n                                                                                # 32 * 32\n    # Padded with 2 on top , 1 in bottom     2 in left and 1 in right\n\n\n    b1 = model.get_layer(\"activation_161\").output                      # 14 * 14\n\n    #model = VGG19(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n    names = [\"input_1\", \"activation_2\", \"activation_4\", \"activation_74\"]\n\n    for feature in [s1 , s2 , s3 , s4]:\n\n        skip_connections.append(feature)\n\n\n    output = model.get_layer(\"activation_161\").output\n    output = ZeroPadding2D( ((1 , 1) , (1, 1)) )(output)                         # 16 * 16 \n\n    return output, skip_connections\n\n\ndef decoder1(inputs, skip_connections):\n    num_filters = [512,256, 128, 64 ]\n    skip_connections.reverse()\n\n    x = inputs\n    for i, f in enumerate(num_filters):\n        x = UpSampling2D((2, 2), interpolation=\"bilinear\")(x)\n        x = Concatenate()([x, skip_connections[i]])\n        x = conv_block(x, f)\n\n    return x\n\ndef output_block(inputs):\n    x = Conv2D(1, 1, padding=\"same\")(inputs)\n    x = Activation(\"sigmoid\")(x)\n    return x\ndef res_block(x, nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n    return res_path\ndef res_block2(x,y,nb_filters, strides):\n    res_path = BatchNormalization()(x)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(res_path)\n\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n\n    res_path = average([y, res_path])#suma doble \n    return res_path\n\n\ndef encoder2(x):\n    to_decoder = []\n    \n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(main_path)\n\n    shortcut = Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1))(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    main_path = add([shortcut, hpath])#suma corta\n\n    to_decoder.append(main_path)\n\n\n    s1 = Conv2D(filters=128, kernel_size=(1, 1), strides=(2, 2))(x)\n    s1 = BatchNormalization()(s1)\n\n    main_path = res_block2(main_path,s1, [128, 128], [(2, 2), (1, 1)]) \n    to_decoder.append(main_path)\n\n    main_path = res_block(main_path, [256, 256], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    s2 = Conv2D(filters=512, kernel_size=(1, 1), strides=(4, 4))(to_decoder[1])\n    s2 = BatchNormalization()(s2)\n\n    main_path = res_block2(main_path,s2, [512, 512], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n    \n    s3 = Conv2D(filters=1024, kernel_size=(1, 1), strides=(8, 8))(to_decoder[1])\n    s3 = BatchNormalization()(s3)\n\n    main_path = res_block2(main_path,s3, [1024, 1024], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    return to_decoder[-1] , to_decoder[0:-1]\n\ndef decoder2(x, from_encoder1 , from_encoder2):\n    \n    from_encoder2.reverse()\n    \n    main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\")(x)# 32 * 32\n    main_path1 = concatenate([main_path, from_encoder1[0] ,from_encoder2[0]], axis=3)\n    main_path = res_block(main_path1, [512, 512], [(1, 1), (1, 1)])\n\n    main_path = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(main_path)###64x64\n    main_path = concatenate([main_path, from_encoder1[1] ,from_encoder2[1] ], axis=3)#\n    u1 = UpSampling2D(size=(2, 2))(main_path1)#\n    u1 = Conv2D(256, kernel_size=(1, 1),strides=(1, 1))(u1)\n    u1 = BatchNormalization()(u1)   \n    #u1 = Activation(activation='relu')(u1)\n    u1 = Dropout(0.2)(u1)\n    \n    \n    main_path = res_block2(main_path,u1, [256, 256], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\")(main_path)#128x128\n    main_path2 = concatenate([main_path, from_encoder1[2] , from_encoder2[2]], axis=3)#\n    main_path = res_block(main_path2, [128, 128], [(1, 1), (1, 1)])#\n\n    main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\" )(main_path)#256x256\n    main_path = concatenate([main_path, from_encoder1[3] , from_encoder2[3]], axis=3)#256x256\n\n    u2 = UpSampling2D(size=(2,2) , interpolation=\"bilinear\" )(main_path2)#\n    u2 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u2)#\n    u2 = BatchNormalization()(u2)\n    #u2 = Activation(activation='relu')(u2)\n    u2 = Dropout(0.2)(u2)\n    main_path3 = res_block2(main_path,u2, [64, 64], [(1, 1), (1, 1)])#256x256\n\n    \n    #main_path = UpSampling2D(size=(2, 2) , interpolation=\"bilinear\" )(main_path3)#512 * 512\n    #main_path = concatenate([main_path, from_encoder1[5] , from_encoder2[5]  ], axis=3)# 512 * 512\n    \n    #u3 = UpSampling2D(size=(2,2) , interpolation=\"bilinear\")(main_path3)#\n    #u3 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u3)#\n    #u3 = BatchNormalization()(u3)\n    #u3 = Activation(activation='relu')(u3)\n    #u3 = Dropout(0.2)(u3)\n    \n    main_path = res_block(main_path3, [ 64, 64], [(1, 1), (1, 1)])#\n\n\n    return main_path\n\n\n\ndef build_model(input_shape):\n    inputs = Input(input_shape)\n    x, skip_1 = encoder1(inputs)\n    x = ASPP(x, 64)\n    x = decoder1(x, skip_1)\n    output1 = output_block(x)\n\n    x = inputs * output1\n\n    x , skip_2 = encoder2(x)\n    x = ASPP(x, 64)\n    x  = decoder2(x, skip_1, skip_2)\n    output2 = output_block(x)\n\n    \n    model = Model(inputs, output2)\n    return model\n\n\nif __name__ == \"__main__\":\n    input_shape = (512, 512, 3)\n    model = build_model(input_shape)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:12.405337Z","iopub.execute_input":"2021-07-26T17:37:12.406164Z","iopub.status.idle":"2021-07-26T17:37:28.825574Z","shell.execute_reply.started":"2021-07-26T17:37:12.406119Z","shell.execute_reply":"2021-07-26T17:37:28.824675Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n219062272/219055592 [==============================] - 8s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimg_w  , img_h = 512 , 512\nn_label = 1\n#model =res2unet(lrate=8.00E-04,pretrained_weights=None)\n#model = unet(lrate=1e-4,ls=2)# WBCE\n#model = att_r2_unet(img_w, img_h, n_label, data_format='channels_last')\nmetrics = [\"acc\" , recall , precision  , iou_coeff ,dice_loss]\nmodel_checkpoint1 = keras.callbacks.ModelCheckpoint('Res2Net.hdf5', monitor= 'val_iou_coeff' ,verbose=1,mode='min',save_best_only=True)#para guardar el entranamiento con menor dice loss en validation\n#steps_per_epochnumber= 600 / 2\n#validation_stepsnumber = 200 / 2\n\ncsv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')#respaldo de datos de entranamiento \nmodel.compile(optimizer= opt, loss=ACL5_bce_jaccard_loss , metrics=[\"acc\" , recall , precision , iou_coeff ,dice_loss])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:28.826946Z","iopub.execute_input":"2021-07-26T17:37:28.827341Z","iopub.status.idle":"2021-07-26T17:37:28.861052Z","shell.execute_reply.started":"2021-07-26T17:37:28.827303Z","shell.execute_reply":"2021-07-26T17:37:28.860209Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(train_data,\n                              epochs=32,\n                              steps_per_epoch=300,\n                              validation_steps=100,\n                              validation_data=valid_data,\n                              callbacks=[model_checkpoint1 , ReduceLROnPlateau(monitor='iou_coeff', factor=0.1, patience=4)])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:37:28.862390Z","iopub.execute_input":"2021-07-26T17:37:28.862765Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Found 600 images belonging to 1 classes.\nFound 600 images belonging to 1 classes.\nEpoch 1/32\n(None, 512, 512, 1)\n(None, 512, 512, 1)\n300/300 [==============================] - ETA: 0s - loss: 203505.5066 - acc: 0.9456 - recall: 0.5761 - precision: 0.3662 - iou_coeff: 0.2279 - dice_loss: 0.6653Found 200 images belonging to 1 classes.\nFound 200 images belonging to 1 classes.\n(None, 512, 512, 1)\n300/300 [==============================] - 340s 1s/step - loss: 203093.5529 - acc: 0.9457 - recall: 0.5762 - precision: 0.3669 - iou_coeff: 0.2284 - dice_loss: 0.6647 - val_loss: 38031.4961 - val_acc: 0.9867 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_iou_coeff: 0.0013 - val_dice_loss: 0.9976\n\nEpoch 00001: val_iou_coeff improved from inf to 0.00126, saving model to Res2Net.hdf5\nEpoch 2/32\n300/300 [==============================] - 302s 1s/step - loss: 20251.6482 - acc: 0.9930 - recall: 0.6647 - precision: 0.7893 - iou_coeff: 0.5460 - dice_loss: 0.3065 - val_loss: 22007.8926 - val_acc: 0.9919 - val_recall: 0.4140 - val_precision: 0.9148 - val_iou_coeff: 0.3967 - val_dice_loss: 0.4606\n\nEpoch 00002: val_iou_coeff did not improve from 0.00126\nEpoch 3/32\n300/300 [==============================] - 302s 1s/step - loss: 17097.9330 - acc: 0.9940 - recall: 0.7188 - precision: 0.8233 - iou_coeff: 0.6136 - dice_loss: 0.2475 - val_loss: 120040.1172 - val_acc: 0.9551 - val_recall: 0.6653 - val_precision: 0.6861 - val_iou_coeff: 0.4804 - val_dice_loss: 0.3865\n\nEpoch 00003: val_iou_coeff did not improve from 0.00126\nEpoch 4/32\n300/300 [==============================] - 302s 1s/step - loss: 16289.3868 - acc: 0.9942 - recall: 0.7292 - precision: 0.8187 - iou_coeff: 0.6235 - dice_loss: 0.2423 - val_loss: 31065.8984 - val_acc: 0.9886 - val_recall: 0.6110 - val_precision: 0.8584 - val_iou_coeff: 0.5481 - val_dice_loss: 0.3078\n\nEpoch 00004: val_iou_coeff did not improve from 0.00126\nEpoch 5/32\n300/300 [==============================] - 302s 1s/step - loss: 15829.2966 - acc: 0.9943 - recall: 0.7147 - precision: 0.8139 - iou_coeff: 0.6085 - dice_loss: 0.2534 - val_loss: 15996.2930 - val_acc: 0.9942 - val_recall: 0.6450 - val_precision: 0.8404 - val_iou_coeff: 0.5826 - val_dice_loss: 0.2880\n\nEpoch 00005: val_iou_coeff did not improve from 0.00126\nEpoch 6/32\n300/300 [==============================] - 302s 1s/step - loss: 14134.4166 - acc: 0.9950 - recall: 0.7472 - precision: 0.8542 - iou_coeff: 0.6528 - dice_loss: 0.2169 - val_loss: 14742.7598 - val_acc: 0.9947 - val_recall: 0.7146 - val_precision: 0.8555 - val_iou_coeff: 0.6374 - val_dice_loss: 0.2351\n\nEpoch 00006: val_iou_coeff did not improve from 0.00126\nEpoch 7/32\n300/300 [==============================] - 302s 1s/step - loss: 13741.6528 - acc: 0.9951 - recall: 0.7744 - precision: 0.8459 - iou_coeff: 0.6753 - dice_loss: 0.2002 - val_loss: 14817.9697 - val_acc: 0.9947 - val_recall: 0.6979 - val_precision: 0.8765 - val_iou_coeff: 0.6325 - val_dice_loss: 0.2369\n\nEpoch 00007: val_iou_coeff did not improve from 0.00126\nEpoch 8/32\n300/300 [==============================] - 301s 1s/step - loss: 13684.3472 - acc: 0.9952 - recall: 0.7759 - precision: 0.8555 - iou_coeff: 0.6802 - dice_loss: 0.1950 - val_loss: 15052.1875 - val_acc: 0.9946 - val_recall: 0.6619 - val_precision: 0.8746 - val_iou_coeff: 0.6128 - val_dice_loss: 0.2573\n\nEpoch 00008: val_iou_coeff did not improve from 0.00126\nEpoch 9/32\n300/300 [==============================] - 302s 1s/step - loss: 14061.1398 - acc: 0.9950 - recall: 0.7706 - precision: 0.8408 - iou_coeff: 0.6719 - dice_loss: 0.2057 - val_loss: 14392.5059 - val_acc: 0.9949 - val_recall: 0.7063 - val_precision: 0.8861 - val_iou_coeff: 0.6482 - val_dice_loss: 0.2233\n\nEpoch 00009: val_iou_coeff did not improve from 0.00126\nEpoch 10/32\n300/300 [==============================] - 302s 1s/step - loss: 13311.0340 - acc: 0.9953 - recall: 0.7841 - precision: 0.8554 - iou_coeff: 0.6884 - dice_loss: 0.1897 - val_loss: 14247.6221 - val_acc: 0.9949 - val_recall: 0.7125 - val_precision: 0.8757 - val_iou_coeff: 0.6435 - val_dice_loss: 0.2280\n\nEpoch 00010: val_iou_coeff did not improve from 0.00126\nEpoch 11/32\n300/300 [==============================] - 302s 1s/step - loss: 12700.8583 - acc: 0.9955 - recall: 0.7895 - precision: 0.8573 - iou_coeff: 0.6963 - dice_loss: 0.1845 - val_loss: 14970.8525 - val_acc: 0.9946 - val_recall: 0.6832 - val_precision: 0.8677 - val_iou_coeff: 0.6223 - val_dice_loss: 0.2529\n\nEpoch 00011: val_iou_coeff did not improve from 0.00126\nEpoch 12/32\n300/300 [==============================] - 302s 1s/step - loss: 12854.1353 - acc: 0.9955 - recall: 0.7851 - precision: 0.8498 - iou_coeff: 0.6882 - dice_loss: 0.1919 - val_loss: 14183.5547 - val_acc: 0.9949 - val_recall: 0.7145 - val_precision: 0.8891 - val_iou_coeff: 0.6492 - val_dice_loss: 0.2258\n\nEpoch 00012: val_iou_coeff did not improve from 0.00126\nEpoch 13/32\n300/300 [==============================] - 301s 1s/step - loss: 13141.6859 - acc: 0.9954 - recall: 0.7913 - precision: 0.8597 - iou_coeff: 0.6971 - dice_loss: 0.1834 - val_loss: 14480.7861 - val_acc: 0.9948 - val_recall: 0.7134 - val_precision: 0.8848 - val_iou_coeff: 0.6487 - val_dice_loss: 0.2211\n\nEpoch 00013: val_iou_coeff did not improve from 0.00126\nEpoch 14/32\n300/300 [==============================] - 302s 1s/step - loss: 13665.6684 - acc: 0.9952 - recall: 0.7658 - precision: 0.8522 - iou_coeff: 0.6765 - dice_loss: 0.2013 - val_loss: 14767.2100 - val_acc: 0.9947 - val_recall: 0.6960 - val_precision: 0.8722 - val_iou_coeff: 0.6319 - val_dice_loss: 0.2385\n\nEpoch 00014: val_iou_coeff did not improve from 0.00126\nEpoch 15/32\n300/300 [==============================] - 301s 1s/step - loss: 13366.9685 - acc: 0.9953 - recall: 0.7755 - precision: 0.8665 - iou_coeff: 0.6898 - dice_loss: 0.1892 - val_loss: 14503.5537 - val_acc: 0.9948 - val_recall: 0.7120 - val_precision: 0.8735 - val_iou_coeff: 0.6419 - val_dice_loss: 0.2314\n\nEpoch 00015: val_iou_coeff did not improve from 0.00126\nEpoch 16/32\n300/300 [==============================] - 302s 1s/step - loss: 13152.6070 - acc: 0.9954 - recall: 0.7802 - precision: 0.8620 - iou_coeff: 0.6898 - dice_loss: 0.1886 - val_loss: 14631.2354 - val_acc: 0.9948 - val_recall: 0.7248 - val_precision: 0.8803 - val_iou_coeff: 0.6563 - val_dice_loss: 0.2167\n\nEpoch 00016: val_iou_coeff did not improve from 0.00126\nEpoch 17/32\n102/300 [=========>....................] - ETA: 3:02 - loss: 12926.5705 - acc: 0.9955 - recall: 0.7965 - precision: 0.8618 - iou_coeff: 0.7003 - dice_loss: 0.1808","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/testdata/Test'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + 'test_image_folder'\ntest_label_folder =\"label\"\n\n\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes , test_path = Test_path , save_path = './' )\n\ntest_data = dp.testGenerator()\n\nimport os\n  \n# Directory\ndirectory = \"RESULT_R2NET\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef read_image(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (512, 512))\n    x = x/255.0\n    return x\n\ndef read_mask(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, ( 512 , 512))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\n\n\n\ndef tf_parse(imagepath , maskpath):\n  def _parse(imagepath , maskpath):\n    x = read_image(imagepath)\n    y = read_mask(maskpath)\n\n    return x , y\n\n  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n  x.set_shape([512 , 512 , 3])\n  y.set_shape([512, 512, 1])\n\n  return x , y \n\n\ndef tf_dataset( imagepath , maskpath , batch = 2):\n  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n  dataset = dataset.map(tf_parse)\n  dataset = dataset.batch(batch)\n  dataset = dataset.repeat()\n  return dataset\n\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n\n    test_steps = (len(test_x)//batch_size)\n    if len(test_x) % batch_size != 0:\n        test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n\n    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n        x = read_image(x)\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_DenseNet210WithDropout/{i}.png\", image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}